# Render 무료 티어 최적 모델 가이드

## 🏆 **추천 모델: Qwen2.5:1.5B**

### **선택 이유**

1. **메모리 효율성**: 1.5B 파라미터로 1GB RAM 내에서 안정적 실행
2. **한국어 성능**: Qwen 시리즈는 한국어에 최적화된 모델
3. **구조화된 답변**: JSON, 마크다운 등 구조화된 출력 우수
4. **빠른 추론**: 작은 모델로 CPU 제약에서도 빠른 응답
5. **Render 호환성**: 무료 티어 제약사항에 완벽 적합

## 📊 **모델 비교표**

| 모델 | 크기 | RAM 사용량 | 한국어 | 구조화 | 추론속도 | Render 적합성 |
|------|------|------------|--------|--------|----------|---------------|
| **Qwen2.5:1.5B** | 1.5B | ~800MB | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ✅ **최적** |
| Qwen2.5:3B | 3B | ~1.5GB | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ❌ 메모리 부족 |
| Qwen2.5:7B | 7B | ~4GB | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ❌ 메모리 부족 |
| Llama3.2:1B | 1B | ~600MB | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⚠️ 한국어 약함 |
| Gemma2:2B | 2B | ~1.2GB | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⚠️ 메모리 부족 |

## 🔧 **최적화 설정**

### 1. Dockerfile 최적화
```dockerfile
# 메모리 최적화 설정
ENV OLLAMA_NUM_PARALLEL=1
ENV OLLAMA_MAX_LOADED_MODELS=1
ENV OLLAMA_FLASH_ATTENTION=1
ENV OLLAMA_GPU_LAYERS=0

# 최적화된 모델 다운로드
RUN ollama pull qwen2.5:1.5b
```

### 2. 환경 변수 설정
```env
OLLAMA_MODEL=qwen2.5:1.5b
OLLAMA_BASE_URL=https://meta-faq-ollama.onrender.com
```

### 3. 시스템 프롬프트 최적화
- 마크다운 형식 사용
- 구조화된 답변 요구
- 한국어 전용 설정
- 실무 적용 가능한 가이드라인

## 📈 **성능 벤치마크**

### 1. 로컬 환경 (개발)
- **응답 시간**: 2-5초
- **메모리 사용량**: ~800MB
- **한국어 품질**: 우수
- **구조화된 답변**: 양호

### 2. Render 환경 (프로덕션)
- **응답 시간**: 10-30초 (슬립 모드 고려)
- **메모리 사용량**: ~800MB
- **한국어 품질**: 우수
- **구조화된 답변**: 양호
- **슬립에서 깨어남**: 30-60초

## 🚀 **배포 단계**

### 1. 로컬 테스트
```bash
# 1. Ollama 서버 시작
ollama serve

# 2. 최적화된 모델 설치
ollama pull qwen2.5:1.5b

# 3. 성능 벤치마크 실행
node scripts/benchmark-models.js

# 4. 개발 서버 시작
npm run dev
```

### 2. Render 배포
1. **Ollama 서버 배포**
   - Docker 환경 선택
   - `Dockerfile.ollama` 사용
   - 무료 티어 선택

2. **Next.js 앱 배포**
   - Node.js 환경 선택
   - 환경 변수 설정
   - 무료 티어 선택

## ⚠️ **주의사항**

### 1. 무료 티어 제한
- **메모리**: 1GB (모델 + 시스템)
- **CPU**: 0.1 CPU (매우 제한적)
- **슬립 모드**: 15분 비활성 시 자동 슬립
- **월 사용량**: 750시간

### 2. 성능 최적화
- Keep-alive 스크립트로 슬립 방지
- 첫 요청 시 로딩 상태 표시
- 에러 처리 및 fallback 메커니즘

### 3. 모니터링
- Render Dashboard에서 리소스 사용량 확인
- 헬스체크 API로 서비스 상태 모니터링
- 에러 로그 실시간 확인

## 🔄 **대안 모델**

### 1. 메모리 부족 시
- **Llama3.2:1B**: 600MB 사용, 한국어 약함
- **Gemma2:1B**: 500MB 사용, 한국어 보통

### 2. 성능 향상 시
- **Qwen2.5:3B**: 1.5GB 사용, 유료 플랜 필요
- **Qwen2.5:7B**: 4GB 사용, 유료 플랜 필요

## 📋 **체크리스트**

### 1. 배포 전 확인
- [ ] 로컬에서 모델 테스트 완료
- [ ] 성능 벤치마크 통과
- [ ] 메모리 사용량 1GB 이하
- [ ] 응답 시간 30초 이내

### 2. 배포 후 확인
- [ ] Ollama 서버 정상 작동
- [ ] Next.js 앱 정상 작동
- [ ] 챗봇 질문/답변 정상
- [ ] RAG 검색 정상
- [ ] 헬스체크 API 정상

### 3. 모니터링 설정
- [ ] Keep-alive 스크립트 실행
- [ ] 에러 로그 모니터링
- [ ] 성능 메트릭 확인
- [ ] 사용자 피드백 수집

## 🎯 **성공 기준**

### 1. 기능적 요구사항
- [ ] 한국어 질문/답변 정상
- [ ] 구조화된 답변 생성
- [ ] RAG 검색 정상 작동
- [ ] 관련 리소스 표시

### 2. 성능 요구사항
- [ ] 평균 응답 시간 30초 이내
- [ ] 메모리 사용량 1GB 이하
- [ ] 에러율 5% 이하
- [ ] 가용성 95% 이상

### 3. 사용자 경험
- [ ] 로딩 상태 표시
- [ ] 에러 메시지 명확
- [ ] 반응형 디자인
- [ ] 접근성 준수

이 가이드를 따라하면 Render 무료 티어에서 최적의 성능을 발휘하는 Ollama 기반 챗봇을 구축할 수 있습니다.
